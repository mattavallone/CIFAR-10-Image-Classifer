{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR-10 Image Classifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "dYl9vHLKApuf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CS-GY 9223-D: Deep Learning Homework 1\n",
        "Due on Tuesday, 19th February 2019, 11:55 PM\n",
        "\n",
        "This homework can be done in pairs. Everyone must submit on NYU Classes individually.\n",
        "\n",
        "Write down the UNIs (NetIDs) of your group (if applicable)\n",
        "\n",
        "Member 1: Matthew Avallone, mva271\n",
        "\n",
        "Member 2: Victor Zheng, vz365"
      ]
    },
    {
      "metadata": {
        "id": "x_s6vogWApul",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision.transforms import transforms\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F5TQ6ipJApux",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ]
    },
    {
      "metadata": {
        "id": "bUAlJ0OJApu0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.transforms import transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nUxZWE896mRc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Downloading the Datasets"
      ]
    },
    {
      "metadata": {
        "id": "5vhNZ0ttApu9",
        "colab_type": "code",
        "outputId": "2708bf93-f225-40d8-f799-93f77048924c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "valid_size = 0.1 # 10% of training data\n",
        "\n",
        "train_transform = transforms.Compose(\n",
        "    [transforms.RandomCrop(size=[32,32], padding=4), # adding some data augmentation\n",
        "     transforms.RandomHorizontalFlip(), transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=train_transform)\n",
        "validset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dsss9W7C6tHQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Loading the Datasets"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "mvrRrsWyhPYs",
        "outputId": "2fd6eb52-e757-4487-eb1c-fb87a25faa81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "num_train = len(trainset)\n",
        "indices = list(range(num_train))\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "np.random.shuffle(indices) # selecting random examples for training/validation\n",
        "\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "trainloader = DataLoader(trainset,batch_size=64, sampler=train_sampler)\n",
        "validloader = DataLoader(validset,batch_size=64, sampler=valid_sampler)\n",
        "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
        "print(len(trainloader.dataset), len(validloader.dataset)*valid_size, len(testloader.dataset))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000 5000.0 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MjHC05KlApvD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "metadata": {
        "id": "KDZC1qP0ApvF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN,self).__init__()\n",
        "        #network architecture\n",
        "        #Input is originally 32x32 RGB images (3 channels)\n",
        "        \n",
        "#         self.conv1 = nn.Conv2d(3, 12, 3, stride=1, padding=1)\n",
        "#         self.conv2 = nn.Conv2d(12, 12, 3, stride=1, padding=1)\n",
        "#         self.conv3 = nn.Conv2d(12, 24, 3, stride=1, padding=1)\n",
        "#         self.conv4 = nn.Conv2d(24, 24, 3, stride=1, padding=1)\n",
        "#         self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "#         self.dp = nn.Dropout(p=0.5)\n",
        "#         self.bn1 = nn.BatchNorm2d(12)\n",
        "#         self.bn2 = nn.BatchNorm2d(24)\n",
        "#         self.activ = nn.ReLU()          \n",
        "#         self.fc = nn.Linear(16 * 16 * 24, 10)\n",
        "#       Could not exceed 70% test accuracy --> added more layers, filters and regularization\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 32, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(64, 64, 3, padding=1)   \n",
        "        self.conv5 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.conv6 = nn.Conv2d(128, 128, 3, padding=1)\n",
        "        \n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "        \n",
        "        self.dp1 = nn.Dropout(p=0.2)\n",
        "        self.dp2 = nn.Dropout(p=0.5) \n",
        "        \n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        \n",
        "        self.activ = nn.ReLU()       \n",
        "        \n",
        "        self.fc = nn.Linear(4 * 4 * 128, 10)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        #forward pass\n",
        "        #x is the input\n",
        "        x = self.conv1(x)\n",
        "        x = self.activ(x)\n",
        "#         x = self.bn1(x) batch norm layers caused dropoff in valid accuracy\n",
        "        x = self.conv2(x)\n",
        "        x = self.activ(x)\n",
        "        x = self.bn1(x)        \n",
        "        x = self.pool(x)\n",
        "        x = self.dp1(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.activ(x)\n",
        "#         x = self.bn2(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.activ(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.dp1(x)\n",
        "\n",
        "        x = self.conv5(x)\n",
        "        x = self.activ(x)\n",
        "#         x = self.bn3(x)\n",
        "        x = self.conv6(x)\n",
        "        x = self.activ(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.pool(x)\n",
        "        \n",
        "        x = x.view(-1, 4 * 4 * 128)\n",
        "\n",
        "        x = self.fc(x)\n",
        "#         x = self.dp2(x) training accuracy could not exceed ~40% when used\n",
        "\n",
        "        return F.log_softmax(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JFnJVIZoqsGm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Defining the Optimizer and Loss Function**"
      ]
    },
    {
      "metadata": {
        "id": "XnREXHI6qurP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = CNN()\n",
        "model.cuda() # Moving the CNN onto a GPU\n",
        "\n",
        "# loss function\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        " \n",
        "#optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.00001) # generated better training accuracy than SGD and RMSprop\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "# optimizer = optim.RMSprop(model.parameters(), lr=0.001, weight_decay=1e-6)\n",
        "\n",
        "# adaptive learning rate\n",
        "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.3) # adaptive step size (lr=0.0003 when we reach epoch 25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YhBoK4xOqlPK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Training the Model**"
      ]
    },
    {
      "metadata": {
        "id": "5dLKPr_Aqkfy",
        "colab_type": "code",
        "outputId": "dd92fe3a-5d64-4752-ab69-0689ec80ee8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 961
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "num_epochs = 50\n",
        "model.train()\n",
        "\n",
        "i = 0\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for X, y in trainloader:\n",
        "\n",
        "        #get the inputs\n",
        "        X, y = X.cuda(), y.cuda()\n",
        "\n",
        "        #please ensure that the data is in the appropriate tensor form as required.\n",
        "        # zero the parameter gradients\n",
        "        # forward pass\n",
        "        # backward pass\n",
        "        # optimize the weights\n",
        "        # print statistics during training\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(X)\n",
        "        loss = loss_function(output, y)\n",
        "        loss.backward()\n",
        "        running_loss = loss.item()\n",
        "        optimizer.step()\n",
        "        y_hat = output.data.max(1)[1]\n",
        "        accuracy = np.sum(y_hat.cpu().numpy()==y.cpu().numpy()) / batch_size*100\n",
        "        \n",
        "        \n",
        "        if i % 700 == 0:\n",
        "            model.eval()\n",
        "            correct = 0.0\n",
        "            for images, labels in validloader:\n",
        "                with torch.no_grad():\n",
        "                    images, labels = images.cuda(), labels.cuda()\n",
        "                    # Predict classes using images from the test set\n",
        "                    outputs = model(images)\n",
        "                    y_pred = outputs.data.max(1)[1]\n",
        "                    correct += y_pred.eq(labels.data).sum()\n",
        "                    \n",
        "            print('Epoch: {}\\tTrain Step: {}\\tloss: {:.3f}\\tTrain Accuracy: {:.1f}\\tValid Accuracy: {:.1f}'.format(epoch, i, loss.item(), accuracy, 100.0*correct / (len(validloader.dataset)*valid_size)))\n",
        "            model.train()\n",
        "\n",
        "        i+= 1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:73: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\tTrain Step: 0\tloss: 2.381\tTrain Accuracy: 15.6\tValid Accuracy: 9.0\n",
            "Epoch: 0\tTrain Step: 700\tloss: 0.975\tTrain Accuracy: 57.8\tValid Accuracy: 64.0\n",
            "Epoch: 1\tTrain Step: 1400\tloss: 0.726\tTrain Accuracy: 70.3\tValid Accuracy: 72.0\n",
            "Epoch: 2\tTrain Step: 2100\tloss: 0.869\tTrain Accuracy: 71.9\tValid Accuracy: 73.0\n",
            "Epoch: 3\tTrain Step: 2800\tloss: 0.832\tTrain Accuracy: 73.4\tValid Accuracy: 77.0\n",
            "Epoch: 4\tTrain Step: 3500\tloss: 0.933\tTrain Accuracy: 59.4\tValid Accuracy: 79.0\n",
            "Epoch: 5\tTrain Step: 4200\tloss: 0.827\tTrain Accuracy: 76.6\tValid Accuracy: 80.0\n",
            "Epoch: 6\tTrain Step: 4900\tloss: 0.640\tTrain Accuracy: 75.0\tValid Accuracy: 80.0\n",
            "Epoch: 7\tTrain Step: 5600\tloss: 0.776\tTrain Accuracy: 73.4\tValid Accuracy: 81.0\n",
            "Epoch: 8\tTrain Step: 6300\tloss: 0.750\tTrain Accuracy: 73.4\tValid Accuracy: 82.0\n",
            "Epoch: 9\tTrain Step: 7000\tloss: 0.468\tTrain Accuracy: 79.7\tValid Accuracy: 82.0\n",
            "Epoch: 10\tTrain Step: 7700\tloss: 0.639\tTrain Accuracy: 79.7\tValid Accuracy: 84.0\n",
            "Epoch: 11\tTrain Step: 8400\tloss: 0.432\tTrain Accuracy: 87.5\tValid Accuracy: 84.0\n",
            "Epoch: 12\tTrain Step: 9100\tloss: 0.614\tTrain Accuracy: 79.7\tValid Accuracy: 83.0\n",
            "Epoch: 13\tTrain Step: 9800\tloss: 0.429\tTrain Accuracy: 82.8\tValid Accuracy: 84.0\n",
            "Epoch: 14\tTrain Step: 10500\tloss: 0.525\tTrain Accuracy: 79.7\tValid Accuracy: 84.0\n",
            "Epoch: 15\tTrain Step: 11200\tloss: 0.233\tTrain Accuracy: 93.8\tValid Accuracy: 85.0\n",
            "Epoch: 16\tTrain Step: 11900\tloss: 0.417\tTrain Accuracy: 84.4\tValid Accuracy: 84.0\n",
            "Epoch: 17\tTrain Step: 12600\tloss: 0.580\tTrain Accuracy: 78.1\tValid Accuracy: 85.0\n",
            "Epoch: 18\tTrain Step: 13300\tloss: 0.369\tTrain Accuracy: 85.9\tValid Accuracy: 85.0\n",
            "Epoch: 19\tTrain Step: 14000\tloss: 0.340\tTrain Accuracy: 92.2\tValid Accuracy: 85.0\n",
            "Epoch: 20\tTrain Step: 14700\tloss: 0.460\tTrain Accuracy: 82.8\tValid Accuracy: 86.0\n",
            "Epoch: 21\tTrain Step: 15400\tloss: 0.348\tTrain Accuracy: 90.6\tValid Accuracy: 86.0\n",
            "Epoch: 22\tTrain Step: 16100\tloss: 0.371\tTrain Accuracy: 85.9\tValid Accuracy: 85.0\n",
            "Epoch: 23\tTrain Step: 16800\tloss: 0.284\tTrain Accuracy: 92.2\tValid Accuracy: 87.0\n",
            "Epoch: 24\tTrain Step: 17500\tloss: 0.273\tTrain Accuracy: 93.8\tValid Accuracy: 86.0\n",
            "Epoch: 25\tTrain Step: 18200\tloss: 0.360\tTrain Accuracy: 82.8\tValid Accuracy: 86.0\n",
            "Epoch: 26\tTrain Step: 18900\tloss: 0.396\tTrain Accuracy: 84.4\tValid Accuracy: 86.0\n",
            "Epoch: 27\tTrain Step: 19600\tloss: 0.406\tTrain Accuracy: 87.5\tValid Accuracy: 86.0\n",
            "Epoch: 28\tTrain Step: 20300\tloss: 0.300\tTrain Accuracy: 89.1\tValid Accuracy: 86.0\n",
            "Epoch: 29\tTrain Step: 21000\tloss: 0.282\tTrain Accuracy: 92.2\tValid Accuracy: 86.0\n",
            "Epoch: 30\tTrain Step: 21700\tloss: 0.323\tTrain Accuracy: 87.5\tValid Accuracy: 86.0\n",
            "Epoch: 31\tTrain Step: 22400\tloss: 0.164\tTrain Accuracy: 96.9\tValid Accuracy: 87.0\n",
            "Epoch: 32\tTrain Step: 23100\tloss: 0.303\tTrain Accuracy: 89.1\tValid Accuracy: 87.0\n",
            "Epoch: 33\tTrain Step: 23800\tloss: 0.186\tTrain Accuracy: 93.8\tValid Accuracy: 87.0\n",
            "Epoch: 34\tTrain Step: 24500\tloss: 0.490\tTrain Accuracy: 78.1\tValid Accuracy: 87.0\n",
            "Epoch: 35\tTrain Step: 25200\tloss: 0.377\tTrain Accuracy: 84.4\tValid Accuracy: 87.0\n",
            "Epoch: 36\tTrain Step: 25900\tloss: 0.330\tTrain Accuracy: 92.2\tValid Accuracy: 87.0\n",
            "Epoch: 37\tTrain Step: 26600\tloss: 0.283\tTrain Accuracy: 87.5\tValid Accuracy: 87.0\n",
            "Epoch: 38\tTrain Step: 27300\tloss: 0.274\tTrain Accuracy: 90.6\tValid Accuracy: 87.0\n",
            "Epoch: 39\tTrain Step: 28000\tloss: 0.330\tTrain Accuracy: 85.9\tValid Accuracy: 87.0\n",
            "Epoch: 40\tTrain Step: 28700\tloss: 0.314\tTrain Accuracy: 87.5\tValid Accuracy: 87.0\n",
            "Epoch: 41\tTrain Step: 29400\tloss: 0.253\tTrain Accuracy: 92.2\tValid Accuracy: 87.0\n",
            "Epoch: 42\tTrain Step: 30100\tloss: 0.247\tTrain Accuracy: 89.1\tValid Accuracy: 87.0\n",
            "Epoch: 43\tTrain Step: 30800\tloss: 0.252\tTrain Accuracy: 93.8\tValid Accuracy: 87.0\n",
            "Epoch: 44\tTrain Step: 31500\tloss: 0.478\tTrain Accuracy: 85.9\tValid Accuracy: 87.0\n",
            "Epoch: 45\tTrain Step: 32200\tloss: 0.242\tTrain Accuracy: 92.2\tValid Accuracy: 87.0\n",
            "Epoch: 46\tTrain Step: 32900\tloss: 0.283\tTrain Accuracy: 90.6\tValid Accuracy: 87.0\n",
            "Epoch: 47\tTrain Step: 33600\tloss: 0.369\tTrain Accuracy: 85.9\tValid Accuracy: 87.0\n",
            "Epoch: 48\tTrain Step: 34300\tloss: 0.212\tTrain Accuracy: 89.1\tValid Accuracy: 87.0\n",
            "Epoch: 49\tTrain Step: 35000\tloss: 0.226\tTrain Accuracy: 87.5\tValid Accuracy: 87.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MIVrvlw1h-q2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Testing The Model**"
      ]
    },
    {
      "metadata": {
        "id": "BK_ApMUmfh4Y",
        "colab_type": "code",
        "outputId": "1b24bc08-8242-45c2-91cb-53136be1abb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0.0\n",
        "test_pred = torch.LongTensor().cuda()\n",
        "        \n",
        "for X, y in testloader:\n",
        "    with torch.no_grad():\n",
        "        X, y = X.cuda(), y.cuda()\n",
        "        # Predict classes using images from the test set\n",
        "        outputs = model(X)\n",
        "        y_pred = outputs.data.max(1)[1]\n",
        "        test_pred = torch.cat((test_pred, y_pred), dim=0)\n",
        "      \n",
        "        correct += y_pred.eq(y.data).sum()\n",
        "\n",
        "\n",
        "ans2 = test_pred.cpu().numpy()\n",
        "print(ans2.shape)\n",
        "\n",
        "# saving the test results\n",
        "np.save('./ans2-mva271-vz365', ans2)\n",
        "print('Test Accuracy: {:.1f}%'.format(100.0*correct / len(testloader.dataset)))\n",
        "\n",
        "#predict on the test data\n",
        "#save the predictions to ans2-uni.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:73: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(10000,)\n",
            "Test Accuracy: 88.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DDuq_fAH8yFn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Discussion**"
      ]
    },
    {
      "metadata": {
        "id": "m7hGOLEX869m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Model Architecture** \n",
        "\n",
        "The optimal model presented needed 6 convolutional layers, 6 activation layers, 1 fully-connected layer and 1 max-pooling layer. At first, we tried using only 4 convolutional/activation layers (with the rest the same) and could not surpase 70% on the validation data. The activation function was chosen to be ReLU since this is most commonly used with good results. Regularization was also used in the form of 3 instances of batch normalization and 2 instances of droput. We experimented with adding more regularization (commented out in the code) but found that the combination used provided the best results. A log-softmax function is used in the end since the model returns a probalitily for each class and adding a log function to the softmax makes the calculation more stable (turns the division operation inside softmax into subtraction).\n",
        "\n",
        "**Optimizer and Loss Function**\n",
        "\n",
        "The loss function chosen was categorical cross-entropy, since this is commonly used for multiclass classification. Three optimizers were tried, with Adam and RMSprop showing similar and superior results than SGD. We ultimately decided to go with Adam as it increased validation accuracy by 1% over RMSprop. The learning rate was set to default value and weight decay was added as it was seen in several online examples. A weight decay value of 1e-5 and 1e-6 were attempted with no obvious performance differences between the two. An adaptive learning rate was implemented to help the model converge at higher epochs. The step size factor, gamma, was chosen to be 0.3 (next_lr = 30% of current_lr) randomly and seemed to yield sufficient results. We also tried gamma=0.1 and found slight decrease in validation accuracy. \n",
        "\n",
        "**Data**\n",
        "\n",
        "The input data was split into 3 sets: training, validation and test. The validation set was selected randomly from 10% of the training set. The training data was augmented with random cropping and horizontal flipping and all of the data was normalized. This was done to increase the number of examples and helped improve the accuracy of the model by about 20%. The data was broken down into batches of 64 examples, with 50 epochs performed. More epochs were attempted with no improvement to the accuracy.\n",
        "\n"
      ]
    }
  ]
}